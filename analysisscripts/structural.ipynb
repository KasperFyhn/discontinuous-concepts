{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural analysis of discontinuous concepts\n",
    "\n",
    "The purpose of this analysis notebook is to make an in-depth analysis of structures. This makes the basis for a large portion of the corresponding section in the final thesis paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinated concepts\n",
    "\n",
    "Coordinations are the only thing marked in GENIA while CRAFT also contains other types. Therefore, GENIA will be used primarily in this part. The variable `corpus_name` can be set to either `genia` or `craft` to run with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils  # a python module in the same dir as the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = 'acl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the corpus, then extract concepts and their constituent structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ACL 2.0 corpus: 100%|██████████| 300/300 [00:00<00:00, 518.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(utils.ROOT)  # get to the root directory of the project\n",
    "\n",
    "from datautils import dataio, annotations as anno\n",
    "\n",
    "# load the corpora\n",
    "corpus = dataio.load_corpus(corpus_name)\n",
    "\n",
    "# not all docs in genia get Constituent annotations; if so, leave them out\n",
    "if corpus_name.lower() == 'genia':\n",
    "    corpus = [doc for doc in corpus if doc.get_annotations(anno.Constituent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_consts = [(concept, doc.get_annotations_at(concept.span, anno.Constituent)[0])\n",
    "                  for doc in corpus\n",
    "                  for concept in doc.get_annotations(anno.DiscontinuousConcept)\n",
    "                  if len(concept.get_spanned_tokens()) < 10]\n",
    "\n",
    "# for craft, keep only the ones with a conjunction in them\n",
    "temp_list = []\n",
    "for concept, const in concept_consts:\n",
    "    present_pos = {t.mapped_pos() for t in const.get_tokens()}\n",
    "    if 'c' in present_pos:\n",
    "        temp_list.append((concept, const))\n",
    "concept_consts = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each constituent can spit out a treebank structure. Let's do that and put it all in dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def skipped_structure(const, allowed_tokens):\n",
    "    \"\"\"Returns  a treebank structure of a Constituent with a # symbol in front of\n",
    "    all tokens that are in allowed_tokens\"\"\"\n",
    "    # note: craft constituents often have labels like NP-SBJ; keep only the first part\n",
    "    return '(' + const.label.split('-')[0] + ' ' + ' '.join(\n",
    "        skipped_structure(c, allowed_tokens) if isinstance(c, anno.Constituent)\n",
    "        else c.mapped_pos() if c not in allowed_tokens\n",
    "        else '#' + c.mapped_pos()\n",
    "        for c in const.constituents\n",
    "    ) + ')'\n",
    "\n",
    "def unlabeled_structure(const, allowed_tokens):\n",
    "    \"\"\"Returns a simplified treebank structure of a Constituent with no constituent labels\n",
    "    and with a # symbol in front of all tokens that are in allowed_tokens\"\"\"\n",
    "    return_string = '(' + ' '.join(\n",
    "        unlabeled_structure(c, allowed_tokens) if isinstance(c, anno.Constituent)\n",
    "        else c.mapped_pos() if c not in allowed_tokens\n",
    "        else '#' + c.mapped_pos()\n",
    "        for c in const.constituents\n",
    "        if not contains_empty_token(c)\n",
    "    ) + ')'\n",
    "    # remove all cases of single token constituents, e.g. (n)\n",
    "    return re.sub('\\(+(#?.)\\)+', r'\\1', return_string)\n",
    "\n",
    "def contains_empty_token(const):\n",
    "    if not isinstance(const, anno.Constituent):\n",
    "        return False\n",
    "    for c in const.constituents:\n",
    "        if isinstance(c, anno.Token) and c.mapped_pos() == 'Ø':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "full_structures = defaultdict(list)\n",
    "skip_structures = defaultdict(list)\n",
    "collapsed_full_structures = defaultdict(list)\n",
    "collapsed_skip_structures = defaultdict(list)\n",
    "\n",
    "for concept, const in concept_consts:\n",
    "    struct = const.structure()  \n",
    "    full_structures[struct].append(const)\n",
    "    \n",
    "    skip_struct = skipped_structure(const, concept.get_tokens())\n",
    "    skip_structures[skip_struct].append(const)\n",
    "    \n",
    "    collapsed_full = unlabeled_structure(const, {})\n",
    "    collapsed_full_structures[collapsed_full].append(const)\n",
    "    \n",
    "    collapsed_skip = unlabeled_structure(const, concept.get_tokens())\n",
    "    collapsed_skip_structures[collapsed_skip].append(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dcs = len(concept_consts)\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for name, structure_dict in [('full', full_structures),\n",
    "                             ('skip', skip_structures),\n",
    "                             ('col_full', collapsed_full_structures),\n",
    "                             ('col_skip', collapsed_skip_structures)]:\n",
    "    \n",
    "    data_dict = {'struct': [], 'pos-seq': [], 'count': [], '%': [], 'example': []}\n",
    "    \n",
    "    for struct, sample in structure_dict.items():\n",
    "        data_dict['struct'].append(struct)\n",
    "        data_dict['pos-seq'].append(''.join(c for c in struct if c.islower() or c in ','))\n",
    "        data_dict['count'].append(len(sample))\n",
    "        data_dict['%'].append(round(len(sample) / n_dcs * 100, 2))\n",
    "        data_dict['example'].append(sample[0].get_covered_text())\n",
    "        \n",
    "    dfs[name] = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>struct</th>\n",
       "      <th>pos-seq</th>\n",
       "      <th>count</th>\n",
       "      <th>%</th>\n",
       "      <th>example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [struct, pos-seq, count, %, example]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['col_skip'].sort_values('count', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for latex table\n",
    "import re\n",
    "csv_string = dfs['full'].sort_values('count', ascending=False).head(15).to_csv(sep='&')\n",
    "csv_string = re.sub(r'\\n\\d+&', r'  \\\\\\\\ \\\\addlinespace\\n', csv_string)  # line breaks and row numbers\n",
    "csv_string = re.sub('^&', r'', csv_string)  # first &\n",
    "csv_string = re.sub('%', r'\\\\%', csv_string)  # \\% for latex\n",
    "csv_string = re.sub('Ø', r'\\\\O', csv_string)  # \\O for latex\n",
    "csv_string = re.sub('#([A-Za-z\\-]+)', r'\\\\textbf{\\1}', csv_string)  # bold typeface used words \n",
    "csv_string = re.sub('&', r'\\t&\\t', csv_string)  # more space on delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_skip_structures['((#a c a) #n)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can capture more examples under fewer labels if we only use POS-tag sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROSS_COUNTING = ['concept', 'super', 'skip']  # order: concept, super, skip\n",
    "\n",
    "MOD = '[an]'\n",
    "SUPER_SEQ_COLLAPSERS = {\n",
    "    # re.compile('n*(n,?)+cnn+'),\n",
    "    # re.compile('a*(a,?)+caa*n+')\n",
    "    #re.compile('[an]c[an]n'),  # base case: shared head\n",
    "    #re.compile('[an]ncn'),  # base case: shared modifier\n",
    "    #re.compile('([an],)+[an],?c[an]n'),  # exp case: enumeration of modifiers + shared head\n",
    "    #re.compile('[an]+(n,)+n,?cn'),  # exp case: enumeration of heads with shared modifier\n",
    "    #re.compile('[an]c[an][an]+n'),  # exp case: multi-word head\n",
    "    #re.compile('[an]+[an]ncn'),  # exp case: shared pre-modifiers\n",
    "    re.compile(f'({MOD}n?,?)+c{MOD}*n'),  # all\n",
    "}\n",
    "\n",
    "CONCEPT_SEQ_COLLAPSERS = {\n",
    "    re.compile('(a|n)+n')\n",
    "}\n",
    "SAMPLE_CATEGORIES = {'concept', 'super', 'skip', 'cross-count'}\n",
    "\n",
    "def make_sequence(tokens, collapser=None):\n",
    "    sequence = [t.mapped_pos() if not t.pos == '#' else '#'\n",
    "                for t in tokens]\n",
    "    sequence_str = ''.join(sequence)\n",
    "    if not collapser:\n",
    "        return sequence_str\n",
    "\n",
    "    for pos_pattern in collapser:\n",
    "        if pos_pattern.fullmatch(sequence_str):\n",
    "            return pos_pattern.pattern\n",
    "    return sequence_str  # sequence did not match a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_concepts = [dc for doc in corpus\n",
    "                 for dc in doc.get_annotations(anno.DiscontinuousConcept)\n",
    "                 if 'c' in {t.mapped_pos() for t in dc.get_spanned_tokens()}\n",
    "                 and len(dc.get_tokens()) < 10]\n",
    "\n",
    "samples_after_category = {}\n",
    "for ct in SAMPLE_CATEGORIES:\n",
    "    samples_after_category[ct] = defaultdict(list)\n",
    "\n",
    "for dc in disc_concepts:\n",
    "    concept_tokens = dc.get_tokens()\n",
    "    all_tokens = dc.get_spanned_tokens()\n",
    "    skip_tokens = [t if t in concept_tokens  # actual token\n",
    "                   else anno.Token(dc.document, (-1, -1), '#')  # skipped token\n",
    "                   for t in all_tokens]\n",
    "\n",
    "    cross_count_type = []\n",
    "    cross_count_example = []\n",
    "\n",
    "    concept_sequence = make_sequence(concept_tokens, CONCEPT_SEQ_COLLAPSERS)\n",
    "    concept_text = dc.get_covered_text()\n",
    "    samples_after_category['concept'][concept_sequence].append(dc)\n",
    "    if 'concept' in CROSS_COUNTING:\n",
    "        cross_count_type.append(concept_sequence)\n",
    "        cross_count_example.append(dc)\n",
    "\n",
    "    super_sequence = make_sequence(all_tokens, SUPER_SEQ_COLLAPSERS)\n",
    "    super_text = dc.get_spanned_text()\n",
    "    samples_after_category['super'][super_sequence].append(dc)\n",
    "    if 'super' in CROSS_COUNTING:\n",
    "        cross_count_type.append(super_sequence)\n",
    "        cross_count_example.append(dc)\n",
    "\n",
    "    skip_sequence = make_sequence(skip_tokens)\n",
    "    samples_after_category['skip'][skip_sequence].append(dc)\n",
    "    if 'skip' in CROSS_COUNTING:\n",
    "        cross_count_type.append(skip_sequence)\n",
    "        cross_count_example.append(dc)\n",
    "\n",
    "    samples_after_category['cross-count'][tuple(cross_count_type)].append(\n",
    "        tuple(cross_count_example)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'type': [], 'count': [], '%': [], 'example': []}\n",
    "for type_, items in samples_after_category['super'].items():\n",
    "    data_dict['type'].append(type_)\n",
    "    data_dict['count'].append(len(items))\n",
    "    data_dict['%'].append(round(len(items) / len(disc_concepts) * 100, 2))\n",
    "    data_dict['example'].append(items[0].get_spanned_text())\n",
    "    \n",
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('count', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in samples_after_category['super']['[an]+[an]ncn']:\n",
    "    print(*(t.get_covered_text() for t in item.get_spanned_tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for exp case: multi-word head: for multiple tokens before/after the conjunction,\n",
    "# do the tokens go with the head or the second modifier?\n",
    "df = dfs['col_full']\n",
    "subset = df[df['pos-seq'].str.contains('^[an]+[an]ncn$')]\n",
    "subset.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about CC's?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = [c for doc in corpus for c in doc.get_annotations(anno.Concept)\n",
    "               if not isinstance(c, anno.DiscontinuousConcept) and not len(c) < 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dist_of_types = Counter(c.pos_sequence() for c in all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_of_types.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_count = sum(v for t, v in dist_of_types.items() if re.match('[an]n', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_count / sum(dist_of_types.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
